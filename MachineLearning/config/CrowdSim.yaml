behaviors:
  CrowdSim:
    trainer_type: ppo

    summary_freq: 10000
    time_horizon: 64
    max_steps: 250000
    keep_checkpoints: 5
    even_checkpoints: false
    checkpoint_interval: 50000
    threaded: false

    #Common parameters
    hyperparameters:
      learning_rate: 3.0e-4
      batch_size: 32
      buffer_size: 100000
      learning_rate_schedule: linear

      #buffer_init_steps: 0
      #init_entcoef: 1.0
      #save_replay_buffer: false
      #tau: 0.005
      #steps_per_update: 1
      #reward_signal_num_update: steps_per_update / 4.0

    network_settings:
      hidden_units: 256
      num_layers: 2
      normalize: true
      vis_encode_type: simple
      conditioning_type: hyper

    #PPO/POCA-specific configurations
    #hyperparameters:
      #beta: 5.0e-3
      #epsilon: 0.2
      #beta_schedule: linear
      #epsilon_schedule: linear
      #lambd: 0.95
      #num_epoch: 3
      #shared_critic: false

    #SAC-specific configurations
    #hyperparameters:
      #buffer_init_steps: 0
      #init_entcoef: 1.0
      #save_replay_buffer: false
      #tau: 0.005
      #steps_per_update: 1
      #reward_signal_num_update: steps_per_update / 4.0


    #extrinsic:
      #strength: 1.0
      #gamma: 0.99

    #curiosity:
      #strength: 1.0
      #gamma: 0.99
      #network_settings: 64
      #learning_rate: 3.0e-4